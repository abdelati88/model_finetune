{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iroICq8m7zrG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e"
      ],
      "metadata": {
        "id": "RdbfcAra9AF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "SrZ9_VTf_ofP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n"
      ],
      "metadata": {
        "id": "wLYMdm5BCV_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "!huggingface-cli login --token {hf_token}"
      ],
      "metadata": {
        "id": "aWsQYaRrCZUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/gdrive/MyDrive/finetune\"\n",
        "\n",
        "base_model_id = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-0.5B-Instruct\"\n"
      ],
      "metadata": {
        "id": "8GcaahPhCoT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install json_repair"
      ],
      "metadata": {
        "id": "zsvKlGyzjTZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"numpy>=2.0.0\" \\\n",
        "                     \"transformers==4.48.3\" \\\n",
        "                     \"accelerate>=0.34.2\" \\\n",
        "                     \"tokenizers>=0.20.3\" \\\n",
        "                     \"huggingface_hub>=0.25.2\" \\\n",
        "                     \"safetensors>=0.4.5\" einops\n",
        "# !pip -q install bitsandbytes==0.43.3\n"
      ],
      "metadata": {
        "id": "vDz_C6rFSAg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZK4rfMOJFAH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy\n",
        "!pip install -U transformers accelerate\n"
      ],
      "metadata": {
        "id": "cH61HaJKFFMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "CEUk_rt7Fdht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"\n",
        "ÙƒØ´ÙØª Ù…Ù†ØµØ© â€œwiredâ€ Ø§Ù„Ù…ØªØ®ØµØµØ© Ø¨Ø§Ù„Ù…ÙˆØ§Ø¶ÙŠØ¹ Ø§Ù„ØªÙ‚Ù†ÙŠØ© ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¹Ù† Ø§Ù†ØªØ´Ø§Ø± Ø¸Ø§Ù‡Ø±Ø© Ø³Ø±Ù‚Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ Ø¥Ø° Ø±ØµØ¯Øª Ù…ÙˆÙ‚Ø¹Ù‹Ø§ ÙŠÙ†Ø´Ø± Ù…Ù‚Ø§Ù„Ø§Øª Ù…Ù†Ø³ÙˆØ®Ø©ØŒ ÙˆÙ…Ø¯Ø¹ÙˆÙ…Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ØªØ¸Ù‡Ø± ÙÙŠ Ù…Ø±Ø§ÙƒØ² Ù…ØªÙ‚Ø¯Ù…Ø© Ø¨Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ø¹Ù„Ù‰ Ù…Ø­Ø±Ùƒ â€œØ¬ÙˆØ¬Ù„â€.\n",
        "\n",
        "Ø§Ù„Ù…Ù†ØµØ© Ù‚Ø§Ù„Øª Ø¥Ù† Ø§Ù„Ù‚ØµØ© Ø¨Ø¯Ø£Øª Ø¹Ù†Ø¯Ù…Ø§ Ø¨Ø­Ø« Ø£Ø­Ø¯ Ù…Ø­Ø±Ø±ÙŠÙ‡Ø§ Ø¹Ù† Ù‚ØµØµ Ø¥Ø®Ø¨Ø§Ø±ÙŠØ© ØªØªØ¹Ù„Ù‚ Ø¨ØªØ¹Ø§Ù…Ù„ Ø´Ø±ÙƒØ© â€œØ£Ø¯ÙˆØ¨ÙŠâ€ Ù…Ø¹ Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø³ØªØ®Ø¯Ù…ÙŠÙ†ØŒ ÙˆØªÙˆØ¸ÙŠÙÙ‡ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬Ù‡Ø§ Ù„Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ Ø¹Ù†Ø¯Ù…Ø§ Ø§ÙƒØªØ´Ù Ø£Ù† Ù…Ù‚Ø§Ù„Ù‹Ø§ Ù…Ù†Ø´ÙˆØ±Ù‹Ø§ ÙÙŠ Ø§Ù„Ù…ÙˆÙ‚Ø¹ Ø§Ù„Ø¥Ø®Ø¨Ø§Ø±ÙŠ ÙŠØ¸Ù‡Ø± ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ² Ø§Ù„Ø«Ø§Ù†ÙŠ Ø¯Ø§Ø®Ù„ ØªØ¨ÙˆÙŠØ¨ Ø§Ù„Ø£Ø®Ø¨Ø§Ø± Ù„Ø¯Ù‰ â€œØ¬ÙˆØ¬Ù„â€ØŒ Ø¨ÙŠÙ†Ù…Ø§ ÙŠØ¸Ù‡Ø± Ù…Ù‚Ø§Ù„ Ù…Ù† Ù…Ø¯ÙˆÙ†Ø© Ù…ØºÙ…ÙˆØ±Ø© ØªØ­Ù…Ù„ Ø§Ø³Ù… â€œSyrusâ€ ÙÙŠ Ø§Ù„Ù…Ø±ØªØ¨Ø© Ø§Ù„Ø£ÙˆÙ„Ù‰ØŒ Ø±ØºÙ… Ø£Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ ÙŠØ¸Ù‡Ø± ÙˆÙƒØ£Ù†Ù‡ Ù†Ø³Ø®Ø© Ù…Ø³Ø±ÙˆÙ‚Ø© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ù…Ø¹ ØªØºÙŠÙŠØ±Ø§Øª ØªØ­Ø±ÙŠØ±ÙŠØ© Ø·ÙÙŠÙØ©.\n",
        "\n",
        "ÙˆØ£Ø¶Ø§ÙØª Ø£Ù†Ù‡ Ø¹Ù†Ø¯ Ø§Ù„Ù†Ù‚Ø± Ø¹Ù„Ù‰ Ø§Ù„Ø±Ø§Ø¨Ø· Ø§Ù„ØªØ´Ø¹Ø¨ÙŠ Ù„Ù†ØªÙŠØ¬Ø© Ø§Ù„Ø¨Ø­Ø«ØŒ Ø¸Ù‡Ø± Ù…ÙˆÙ‚Ø¹ Ø¥Ù„ÙƒØªØ±ÙˆÙ†ÙŠ ÙŠØ¹Ø¬ Ø¨Ø§Ù„Ø±Ø³Ø§Ø¦Ù„ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© ÙˆØ§Ù„Ù…Ù‚Ø§Ù„Ø§Øª Ø§Ù„Ù…Ø³Ø±ÙˆÙ‚Ø© Ø§Ù„ØªÙŠ Ø£Ø¹ÙŠØ¯ ØªØ¬Ù…ÙŠØ¹Ù‡Ø§ØŒ ÙˆÙƒØ«ÙŠØ± Ù…Ù†Ù‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø±Ø³ÙˆÙ… Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠØ© Ø§Ù„Ù…Ù†Ø´Ø£Ø© Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ\n",
        "\n",
        "\n",
        "ÙÙŠ Ù…Ù‚Ø§Ù„ Ø§Ù„Ø¨Ø±ÙŠØ¯ Ø§Ù„Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ù‡Ø°Ø§ØŒ Ù†ÙØ³Ø® Ù…Ù‚Ø§Ù„ Ù„Ù„Ù…Ù†ØµØ© Ù†ÙØ³Ù‡Ø§ ÙƒØ§Ù…Ù„Ù‹Ø§ØŒ Ù…Ø¹ ØªØºÙŠÙŠØ±Ø§Øª Ø·ÙÙŠÙØ© ÙÙŠ Ø§Ù„ØµÙŠØ§ØºØ©ØŒ Ø­ØªÙ‰ Ø¥Ù† Ø§Ù„Ø§Ù‚ØªØ¨Ø§Ø³Ø§Øª ÙÙŠ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ø­ÙØ°ÙØª Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ù…Ù†Ø³ÙˆØ®ØŒ ÙˆÙØ±Ø¶ Ø§Ù„Ù…Ù‚Ø§Ù„ Ù†ÙØ³Ù‡ Ø±Ø§Ø¨Ø·Ù‹Ø§ ØªØ´Ø¹Ø¨ÙŠÙ‹Ø§ ÙˆØ­ÙŠØ¯Ù‹Ø§ ÙÙŠ Ø£Ø³ÙÙ„ ØµÙØ­Ø© â€œØ§Ù„ÙˆÙŠØ¨â€ØŒ ÙŠØ¤Ø¯ÙŠ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ø³Ø®Ø© Ø§Ù„Ø£ØµÙ„ÙŠØ© Ù…Ù† Ø§Ù„Ù…Ù‚Ø§Ù„ØŒ ÙƒÙ…ØµØ¯Ø± ÙˆØ­ÙŠØ¯ Ù„Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ÙˆØ§Ø±Ø¯Ø© ÙÙŠÙ‡.\n",
        "\n",
        "ÙˆÙÙ‚ Ø§Ù„ØªÙ‚Ø±ÙŠØ±ØŒ ØªÙˆØ§ØµÙ„Øª â€œwiredâ€ Ù…Ø¹ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø©ØŒ ÙˆØªÙ„Ù‚Øª ØªØ£ÙƒÙŠØ¯Ù‹Ø§ Ø¨Ø£Ù† ÙˆÙƒØ§Ù„Ø© ØªØ³ÙˆÙŠÙ‚ Ø¥ÙŠØ·Ø§Ù„ÙŠØ© Ø£Ù†Ø´Ø£ØªÙ‡Ø§ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù…Øª Ø£Ø¯Ø§Ø© Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠ ÙƒØ¬Ø²Ø¡ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ÙƒØªØ§Ø¨Ø©.\n",
        "\n",
        "ÙˆÙ‚Ø§Ù„ Ù…Ø¯ÙŠØ± Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© Ø§Ù„Ø°ÙŠ ÙŠØ³ØªØ®Ø¯Ù… Ø§Ø³Ù… â€œDaniele Syrusâ€ Ù„Ù€â€wiredâ€ØŒ Ø¥Ù† Ø¹Ù…Ù„ÙŠØ© Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ ØªØªØ¶Ù…Ù† Ø£Ø¯ÙˆØ§Øª Ø°ÙƒØ§Ø¡ Ø§ØµØ·Ù†Ø§Ø¹ÙŠØŒ ØªØªØ±ÙƒØ² Ù…Ù‡Ù…ØªÙ‡Ø§ Ø¹Ù„Ù‰ ØªØ­Ù„ÙŠÙ„ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ù…ØµØ§Ø¯Ø± Ù…ØªØ¹Ø¯Ø¯Ø© â€œÙ…Ø¹ Ø§Ø­ØªØ±Ø§Ù… Ø§Ù„Ù…Ù„ÙƒÙŠØ© Ø§Ù„ÙÙƒØ±ÙŠØ© Ø¯Ø§Ø¦Ù…Ù‹Ø§â€ØŒ Ù…Ù† Ø®Ù„Ø§Ù„ ÙˆØ¶Ø¹ Ø±Ø§Ø¨Ø· Ø§Ù„Ù…Ù‚Ø§Ù„ Ø§Ù„Ø£ØµÙ„ÙŠ Ø£Ø³ÙÙ„ Ø§Ù„Ù…Ù‚Ø§Ù„ Ø¹Ù„Ù‰ Ø§Ù„Ù…ÙˆÙ‚Ø¹.\n",
        "\n",
        "ÙˆØªØ¬Ù†Ø¨Øª Ø´Ø±ÙƒØ© â€œØ¬ÙˆØ¬Ù„â€ Ø§Ù„ØªØ¹Ù„ÙŠÙ‚ Ø¹Ù„Ù‰ ØªÙ‚Ø±ÙŠØ± â€œwiredâ€ØŒ ÙˆÙÙ‚ Ø§Ù„Ù…Ù†ØµØ©ØŒ Ø¨ÙŠÙ†Ù…Ø§ Ø£ÙˆØ¶Ø­Øª Ù…ÙŠØ¬Ø§Ù† ÙØ§Ø±Ù†Ø³ÙˆØ±Ø«ØŒ Ø§Ù„Ù…ØªØ­Ø¯Ø«Ø© Ø¨Ø§Ø³Ù… â€œØ¬ÙˆØ¬Ù„â€ØŒ Ø£Ù† Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„ØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø®Ø§Ù„Ù Ù„Ø¯Ù‰ Ù…Ø­Ø±Ùƒ Ø§Ù„Ø¨Ø­Ø« ØªØ­Ø¸Ø± Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø­ØªÙˆÙ‰ Ù…Ù†Ø®ÙØ¶ Ø§Ù„Ù‚ÙŠÙ…Ø©ØŒ ÙˆØºÙŠØ± Ø£ØµÙ„ÙŠØŒ Ø¨ØºØ±Ø¶ Ø±ÙØ¹ ØªØ±ØªÙŠØ¨ Ù…ÙˆØ§Ù‚Ø¹ â€œØ§Ù„ÙˆÙŠØ¨â€ ÙÙŠ Ù†ØªØ§Ø¦Ø¬ Ø§Ù„Ø¨Ø­Ø« Ù„Ø¯Ù‰ â€œØ¨Ø­Ø« Ø¬ÙˆØ¬Ù„â€ (Google Search)ØŒ Ù…Ø¤ÙƒØ¯Ø© Ø£Ù† Ø§Ù„Ø´Ø±ÙƒØ© ØªØªØ®Ø° Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª Ø¶Ø¯ Ø§Ù„Ù…ÙˆØ§Ù‚Ø¹ Ø§Ù„ØªÙŠ Ù„Ø§ ØªÙ„ØªØ²Ù… Ø¨Ø³ÙŠØ§Ø³Ø§ØªÙ‡Ø§ØŒ ÙˆÙÙ‚ Ø§Ù„Ù…Ù†ØµØ©.\n",
        "\n",
        "Ø¨Ø¯ÙˆØ±Ù‡Ø§ØŒ Ù…Ø¯ÙŠØ±Ø© ØªØ­Ø³ÙŠÙ† Ù…Ø­Ø±ÙƒØ§Øª Ø§Ù„Ø¨Ø­Ø« ÙÙŠ ÙˆÙƒØ§Ù„Ø© Ø§Ù„ØªØ³ÙˆÙŠÙ‚ â€œAmsiveâ€ØŒ Ù„ÙŠÙ„ÙŠ Ø±Ø§ÙŠØŒ Ø°ÙƒØ±Øª Ø£Ù† Ù…Ø´ÙƒÙ„Ø© Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ø§Ù„Ù…Ø²Ø¹Ø¬ Ø§Ù„Ù…Ù†ØªØ´Ø± Ø¹Ù„Ù‰ â€œØ¬ÙˆØ¬Ù„â€ ØµØ¹Ø¨Ø© Ø§Ù„ØªÙØ³ÙŠØ±ØŒ Ù…Ø´ÙŠØ±Ø© Ø¥Ù„Ù‰ Ø£Ù† Ø¨Ø¹Ø¶ Ø§Ù„Ø¹Ù…Ù„Ø§Ø¡ Ø§Ø´ØªÙƒÙˆØ§ Ù…Ù† Ø¥Ø¹Ø§Ø¯Ø© ØµÙŠØ§ØºØ© Ù…Ù‚Ø§Ù„Ø§ØªÙ‡Ù… Ø¨Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø¨Ø´ÙƒÙ„ Ù…Ø´Ø§Ø¨Ù‡ Ù„Ù…Ø­ØªÙˆØ§Ù‡Ù… Ø§Ù„Ø£ØµÙ„ÙŠØŒ ÙˆÙ„ÙƒÙ† Ù…Ø¹ ØªØºÙŠÙŠØ±Ø§Øª Ø·ÙÙŠÙØ©.\n",
        "\n",
        "ÙˆØ¨Ø§Ù„Ù†Ø¸Ø± Ø¥Ù„Ù‰ Ø³ÙŠØ§Ø³Ø§Øª â€œØ¬ÙˆØ¬Ù„â€ØŒ Ø§Ù†ØªÙ‡ÙƒØª Ø§Ù„Ù…Ø¯ÙˆÙ†Ø© Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù†Ø´Ø± ÙÙŠÙ…Ø§ ÙŠØªØ¹Ù„Ù‚ Ø¨Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ø¨Ø± Ø§Ù„Ø¥Ù†ØªØ±Ù†ØªØŒ ÙˆÙÙ‚ â€œwiredâ€ØŒ Ø¥Ø° ØªØ¹ØªØ¨Ø± â€œØ¬ÙˆØ¬Ù„â€ Ù†Ø³Ø® Ø§Ù„Ù…Ø­ØªÙˆÙ‰ Ù…Ù† Ù…ÙˆØ§Ù‚Ø¹ Ø£Ø®Ø±Ù‰ ÙˆØªØ¹Ø¯ÙŠÙ„Ù‡ Ù‚Ù„ÙŠÙ„Ù‹Ø§ ÙÙ‚Ø· Ø®Ø±Ù‚Ù‹Ø§ Ù„Ù„Ù‚ÙˆØ§Ø¹Ø¯.\n",
        "\n",
        "\n",
        "\n",
        ".\"\"\""
      ],
      "metadata": {
        "id": "0KEjmrS2JnVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StoryCategory = Literal[\n",
        "    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "    \"health\", \"entertainment\", \"science\",\n",
        "    \"not_specified\",\"AI\"\n",
        "]\n",
        "\n",
        "EntityType = Literal[\n",
        "    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n",
        "    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n",
        "    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"A fully informative and SEO optimized title of the story.\")\n",
        "\n",
        "    story_keywords: List[str] = Field(..., min_items=1,\n",
        "                                      description=\"Relevant keywords associated with the story.\")\n",
        "\n",
        "    story_summary: List[str] = Field(\n",
        "                                    ..., min_items=1, max_items=5,\n",
        "                                    description=\"Summarized key points about the story (1-5 points).\"\n",
        "                                )\n",
        "\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n",
        "\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n",
        "                                        description=\"List of identified entities in the story.\")\n"
      ],
      "metadata": {
        "id": "Ig4KrIPXJvY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps(\n",
        "                NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "            ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "-G4HUq5ZJ0gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatedStory(BaseModel):\n",
        "    translated_title: str = Field(..., min_length=5, max_length=300,\n",
        "                                  description=\"Suggested translated title of the news story.\")\n",
        "    translated_content: str = Field(..., min_length=5,\n",
        "                                    description=\"Translated content of the news story.\")\n",
        "\n",
        "targeted_lang = \"English\"\n",
        "\n",
        "translation_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are a professional translator.\",\n",
        "            \"You will be provided by an Arabic text.\",\n",
        "            \"You have to translate the text into the `Targeted Language`.\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":  \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Targeted Language:\",\n",
        "            targeted_lang,\n",
        "            \"\",\n",
        "\n",
        "            \"## Translated Story:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "xxeOS9jgJ2j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "target_dir = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "_ = snapshot_download(\n",
        "    repo_id=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    local_dir=target_dir,\n",
        "    local_dir_use_symlinks=False,\n",
        "    token=None,\n",
        ")\n",
        "\n",
        "base_model_id = target_dir\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "FC0z3i8-J6C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "base_model_id = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "sWHWQHRlP3qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "EszmL5AxKBds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "pW1886DqMqns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "Dli8QZhANCbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "KDvkRiQXQgD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "qdtn7UjTUZnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "target_dir = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "_ = snapshot_download(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    local_dir=target_dir,\n",
        "    local_dir_use_symlinks=False,\n",
        "    token=None,\n",
        "\n",
        "print(\"âœ… Model downloaded to:\", target_dir)"
      ],
      "metadata": {
        "id": "u7w9awFsUc2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "Lg20BjwXo5Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id_t = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id_t,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"âœ… Model loaded successfully\")"
      ],
      "metadata": {
        "id": "dmyeClzNkypv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = model.device  # cuda:0\n",
        "\n",
        "\n",
        "# translation_messages = [\n",
        "#   {\"role\": \"system\", \"content\": \"You are a helpful assistant that replies in Arabic.\"},\n",
        "#   {\"role\": \"user\", \"content\": \"ØªØ±Ø¬Ù… Ù„Ù„Ø¬Ù…Ù„Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù„Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ©: Ø§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø±Ø§Ø¦Ø¹.\"}\n",
        "# ]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        top_k=None,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
        "response = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "whJ2ILkGoHDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "torch_dtype = torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=device,\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"âœ… Model loaded successfully\")\n"
      ],
      "metadata": {
        "id": "RvBwsUU3Z6i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade numpy\n"
      ],
      "metadata": {
        "id": "qDoyDEcLX8Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/finetune\"\n",
        "\n",
        "raw_data_path = join(data_dir, \"finetune\",\"datasets\", \"news-sample.jsonl\")\n",
        "\n",
        "raw_data = []\n",
        "with open(raw_data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip() == \"\":\n",
        "            continue\n",
        "        raw_data.append(json.loads(line.strip()))\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "\n",
        "print(f\"âœ… Raw data loaded: {len(raw_data)} samples\")\n"
      ],
      "metadata": {
        "id": "eXBejS9OtIr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data[0]['content']"
      ],
      "metadata": {
        "id": "WKW8SSRiur5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, json_repair, re, os\n",
        "from os.path import join\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/finetune\"\n",
        "save_to = join(data_dir, \"finetune\",\"datasets\", \"sft.jsonl\")\n",
        "os.makedirs(os.path.dirname(save_to), exist_ok=True)\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "def extract_json(text: str):\n",
        "    m = re.search(r\"```json\\s*(\\{.*\\}|\\[.*\\])\\s*```\", text, flags=re.S)\n",
        "    if not m:\n",
        "        m = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, flags=re.S)\n",
        "    if not m:\n",
        "        return None\n",
        "    raw = m.group(1)\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return json_repair.loads(raw)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def ask_teacher(messages, max_new_tokens=512, temperature=0.2):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    device = model.device\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=(temperature is not None and temperature > 0),\n",
        "            temperature=temperature if temperature else None,\n",
        "            top_p=0.9 if temperature else None,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = out[0, input_len:]\n",
        "    reply = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "    in_tok = inputs[\"input_ids\"].numel()\n",
        "    out_tok = gen_ids.numel()\n",
        "    return reply, in_tok, out_tok\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "    sample_details_extraction_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are an NLP data paraser.\",\n",
        "                \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "                \"Generate the output in the same story language.\",\n",
        "                \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "                \"Extract details as mentioned in text.\",\n",
        "                \"Do not generate any introduction or conclusion.\"\n",
        "            ])\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"## Story:\",\n",
        "                story['content'].strip(),\n",
        "                \"\",\n",
        "                \"## Pydantic Details:\",\n",
        "                json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "                \"\",\n",
        "                \"## Story Details:\",\n",
        "                \"```json\"\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        llm_response, in_tok, out_tok = ask_teacher(\n",
        "            sample_details_extraction_messages,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.2\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            torch.cuda.empty_cache()\n",
        "            llm_response, in_tok, out_tok = ask_teacher(\n",
        "                sample_details_extraction_messages,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.2\n",
        "            )\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    llm_resp_dict = extract_json(llm_response)\n",
        "    if not llm_resp_dict:\n",
        "        continue\n",
        "\n",
        "    with open(save_to, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps({\n",
        "            \"id\": ix,\n",
        "            \"story\": story['content'].strip(),\n",
        "            \"task\": \"Extract the story details into a JSON.\",\n",
        "            \"output_scheme\": json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "            \"response\": llm_resp_dict\n",
        "        }, ensure_ascii=False, default=str) + \"\\n\")\n",
        "\n",
        "    ix += 1\n",
        "    prompt_tokens += in_tok\n",
        "    completion_tokens += out_tok\n",
        "\n",
        "    if (ix % 3) == 0:\n",
        "        print(f\"Iteration {ix}: prompt_tokens={prompt_tokens}, completion_tokens={completion_tokens}\")\n",
        "\n",
        "print(f\"âœ… Done. Written samples: {ix}\")\n",
        "print(f\"ðŸ“„ Saved to: {save_to}\")\n"
      ],
      "metadata": {
        "id": "Yxcmay0rvrZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "sft_data_path = join(data_dir, \"finetune\",\"datasets\", \"sft.jsonl\")\n",
        "llm_finetunning_data = []\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join([\n",
        "            \"# Story:\",\n",
        "            rec[\"story\"],\n",
        "\n",
        "            \"# Task:\",\n",
        "            rec[\"task\"],\n",
        "            \"# Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"# Output JSON:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ]),\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"\\n\".join([\n",
        "            \"```json\",\n",
        "            json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "            \"```\"\n",
        "        ]),\n",
        "        \"history\": []\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetunning_data)\n",
        "\n",
        "print(f\"Total data samples: {len(llm_finetunning_data)}\")\n"
      ],
      "metadata": {
        "id": "9nFUDwyAxNM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_sz = 2700\n",
        "\n",
        "train_ds = llm_finetunning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetunning_data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(data_dir, \"finetune\",\"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(data_dir, \"finetune\",\"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(data_dir,\"finetune\", \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ],
      "metadata": {
        "id": "66lKbravc5vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "# resume_from_checkpoint: /gdrive/MyDrive/youtube-resources/llm-finetuning/models/checkpoint-1500\n",
        "output_dir: /gdrive/MyDrive/finetune/finetune/llm-finetuning/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: newsx-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"abdelati88/news-analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n"
      ],
      "metadata": {
        "id": "eaVyoKgEeqk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "0zYx9wU0ilmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "DKrSDqwSi8Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/\n"
      ],
      "metadata": {
        "id": "jDaWDNe6lPrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/examples/train_lora/\n"
      ],
      "metadata": {
        "id": "_DMbvjm3p3jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "mqmJ5KDCqCLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "waG2986JqvCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llama_factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "la28pzfWrX9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/LLaMA-Factory/llama_factory_cli.py train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "WyNmBxNRro5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/LLaMA-Factory/requirements.txt\n"
      ],
      "metadata": {
        "id": "EBHKDdsJrzZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/\n"
      ],
      "metadata": {
        "id": "WP1SXThisJYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"llama_factory_cli.py\"\n"
      ],
      "metadata": {
        "id": "lDV8bD7pvibM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets wandb huggingface_hub\n"
      ],
      "metadata": {
        "id": "7f5QJNsgwn_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/LLaMA-Factory/llama_factory_cli.py train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "oPgotCjB2oM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "id": "ZgMRPpLK2o_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "dpk_1W552tji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/LLaMA-Factory/examples/train_lora\n",
        "\n",
        "news_finetune_yaml = \"\"\"\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "output_dir: /content/drive/MyDrive/finetune/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: newsx-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"abdelati88/news-analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\", \"w\") as f:\n",
        "    f.write(news_finetune_yaml)\n"
      ],
      "metadata": {
        "id": "z7x_hani20pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llama-factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "_bCQg4iO25VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "HncbGbeO29Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "id": "nnKRDM5g3RCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n",
        "!pip install git+https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "id": "VHG_Gz663TZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llama-factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "-iWzwlcg3amF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/LLaMA-Factory/requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "tdyVn1hD3l2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llama-factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "RvlJmdn63rKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "WZ2Xcju03wPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n",
        "huggingface-cli upload --repo_id <your-username>/<model-name> --model_path ./content/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "qGKGBgOM74dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "AScjRzdy7_Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "d6RRt0hO8MF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "yeQSDR0Z8T7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/LLaMA-Factory/src/train.py --config /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "VtE2fqpx8iK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "OZOVfHSFBqLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_id = \"/gdrive/MyDrive/finetune/finetune/llm-finetuning/models\"\n",
        "model.load_adapter(finetuned_model_id)"
      ],
      "metadata": {
        "id": "_zBEiK9HK5n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resp(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "response = generate_resp(translation_messages)"
      ],
      "metadata": {
        "id": "BtmiFA4iLmNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}