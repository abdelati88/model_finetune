{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iroICq8m7zrG"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone --depth 1 https://github.com/hiyouga/LLaMA-Factory.git\n",
        "!cd LLaMA-Factory && pip install -e"
      ],
      "metadata": {
        "id": "RdbfcAra9AF6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "SrZ9_VTf_ofP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n"
      ],
      "metadata": {
        "id": "wLYMdm5BCV_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wandb.login(key=userdata.get('WANDB_API_KEY'))\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "!huggingface-cli login --token {hf_token}"
      ],
      "metadata": {
        "id": "aWsQYaRrCZUF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_dir = \"/gdrive/MyDrive/finetune\"\n",
        "\n",
        "base_model_id = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-0.5B-Instruct\"\n"
      ],
      "metadata": {
        "id": "8GcaahPhCoT2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install json_repair"
      ],
      "metadata": {
        "id": "zsvKlGyzjTZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install -U \"numpy>=2.0.0\" \\\n",
        "                     \"transformers==4.48.3\" \\\n",
        "                     \"accelerate>=0.34.2\" \\\n",
        "                     \"tokenizers>=0.20.3\" \\\n",
        "                     \"huggingface_hub>=0.25.2\" \\\n",
        "                     \"safetensors>=0.4.5\" einops\n",
        "# !pip -q install bitsandbytes==0.43.3\n"
      ],
      "metadata": {
        "id": "vDz_C6rFSAg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import os\n",
        "from os.path import join\n",
        "import random\n",
        "from tqdm.auto import tqdm\n",
        "import requests\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "from typing import List, Optional, Literal\n",
        "from datetime import datetime\n",
        "\n",
        "import json_repair\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch"
      ],
      "metadata": {
        "id": "ZK4rfMOJFAH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U numpy\n",
        "!pip install -U transformers accelerate\n"
      ],
      "metadata": {
        "id": "cH61HaJKFFMt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "torch_dtype = None\n",
        "\n",
        "def parse_json(text):\n",
        "    try:\n",
        "        return json_repair.loads(text)\n",
        "    except:\n",
        "        return None"
      ],
      "metadata": {
        "id": "CEUk_rt7Fdht"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "story = \"\"\"\n",
        "كشفت منصة “wired” المتخصصة بالمواضيع التقنية والذكاء الاصطناعي عن انتشار ظاهرة سرقة المحتوى الإخباري عبر الإنترنت، إذ رصدت موقعًا ينشر مقالات منسوخة، ومدعومة بالذكاء الاصطناعي، تظهر في مراكز متقدمة بنتائج البحث على محرك “جوجل”.\n",
        "\n",
        "المنصة قالت إن القصة بدأت عندما بحث أحد محرريها عن قصص إخبارية تتعلق بتعامل شركة “أدوبي” مع محتوى المستخدمين، وتوظيفه في تدريب نماذجها للذكاء الاصطناعي، عندما اكتشف أن مقالًا منشورًا في الموقع الإخباري يظهر في المركز الثاني داخل تبويب الأخبار لدى “جوجل”، بينما يظهر مقال من مدونة مغمورة تحمل اسم “Syrus” في المرتبة الأولى، رغم أن المقال يظهر وكأنه نسخة مسروقة من المقال الأصلي مع تغييرات تحريرية طفيفة.\n",
        "\n",
        "وأضافت أنه عند النقر على الرابط التشعبي لنتيجة البحث، ظهر موقع إلكتروني يعج بالرسائل العشوائية والمقالات المسروقة التي أعيد تجميعها، وكثير منها باستخدام الرسوم التوضيحية المنشأة بالذكاء الاصطناعي\n",
        "\n",
        "\n",
        "في مقال البريد العشوائي هذا، نُسخ مقال للمنصة نفسها كاملًا، مع تغييرات طفيفة في الصياغة، حتى إن الاقتباسات في المقال الأصلي حُذفت من المقال المنسوخ، وفرض المقال نفسه رابطًا تشعبيًا وحيدًا في أسفل صفحة “الويب”، يؤدي إلى النسخة الأصلية من المقال، كمصدر وحيد للمعلومات الواردة فيه.\n",
        "\n",
        "وفق التقرير، تواصلت “wired” مع مدير المدونة، وتلقت تأكيدًا بأن وكالة تسويق إيطالية أنشأتها، واستخدمت أداة ذكاء اصطناعي كجزء من عملية الكتابة.\n",
        "\n",
        "وقال مدير المدونة الذي يستخدم اسم “Daniele Syrus” لـ”wired”، إن عملية إنشاء المحتوى تتضمن أدوات ذكاء اصطناعي، تتركز مهمتها على تحليل وتجميع المعلومات من مصادر متعددة “مع احترام الملكية الفكرية دائمًا”، من خلال وضع رابط المقال الأصلي أسفل المقال على الموقع.\n",
        "\n",
        "وتجنبت شركة “جوجل” التعليق على تقرير “wired”، وفق المنصة، بينما أوضحت ميجان فارنسورث، المتحدثة باسم “جوجل”، أن سياسات التعامل مع المحتوى المخالف لدى محرك البحث تحظر إنشاء محتوى منخفض القيمة، وغير أصلي، بغرض رفع ترتيب مواقع “الويب” في نتائج البحث لدى “بحث جوجل” (Google Search)، مؤكدة أن الشركة تتخذ إجراءات ضد المواقع التي لا تلتزم بسياساتها، وفق المنصة.\n",
        "\n",
        "بدورها، مديرة تحسين محركات البحث في وكالة التسويق “Amsive”، ليلي راي، ذكرت أن مشكلة المحتوى المزعج المنتشر على “جوجل” صعبة التفسير، مشيرة إلى أن بعض العملاء اشتكوا من إعادة صياغة مقالاتهم بالذكاء الاصطناعي بشكل مشابه لمحتواهم الأصلي، ولكن مع تغييرات طفيفة.\n",
        "\n",
        "وبالنظر إلى سياسات “جوجل”، انتهكت المدونة قواعد النشر فيما يتعلق باستخراج البيانات عبر الإنترنت، وفق “wired”، إذ تعتبر “جوجل” نسخ المحتوى من مواقع أخرى وتعديله قليلًا فقط خرقًا للقواعد.\n",
        "\n",
        "\n",
        "\n",
        ".\"\"\""
      ],
      "metadata": {
        "id": "0KEjmrS2JnVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "StoryCategory = Literal[\n",
        "    \"politics\", \"sports\", \"art\", \"technology\", \"economy\",\n",
        "    \"health\", \"entertainment\", \"science\",\n",
        "    \"not_specified\",\"AI\"\n",
        "]\n",
        "\n",
        "EntityType = Literal[\n",
        "    \"person-male\", \"person-female\", \"location\", \"organization\", \"event\", \"time\",\n",
        "    \"quantity\", \"money\", \"product\", \"law\", \"disease\", \"artifact\", \"not_specified\"\n",
        "]\n",
        "\n",
        "class Entity(BaseModel):\n",
        "    entity_value: str = Field(..., description=\"The actual name or value of the entity.\")\n",
        "    entity_type: EntityType = Field(..., description=\"The type of recognized entity.\")\n",
        "\n",
        "class NewsDetails(BaseModel):\n",
        "    story_title: str = Field(..., min_length=5, max_length=300,\n",
        "                             description=\"A fully informative and SEO optimized title of the story.\")\n",
        "\n",
        "    story_keywords: List[str] = Field(..., min_items=1,\n",
        "                                      description=\"Relevant keywords associated with the story.\")\n",
        "\n",
        "    story_summary: List[str] = Field(\n",
        "                                    ..., min_items=1, max_items=5,\n",
        "                                    description=\"Summarized key points about the story (1-5 points).\"\n",
        "                                )\n",
        "\n",
        "    story_category: StoryCategory = Field(..., description=\"Category of the news story.\")\n",
        "\n",
        "    story_entities: List[Entity] = Field(..., min_items=1, max_items=10,\n",
        "                                        description=\"List of identified entities in the story.\")\n"
      ],
      "metadata": {
        "id": "Ig4KrIPXJvY9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "details_extraction_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are an NLP data paraser.\",\n",
        "            \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "            \"Generate the ouptut in the same story language.\",\n",
        "            \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "            \"Extract details as mentioned in text.\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps(\n",
        "                NewsDetails.model_json_schema(), ensure_ascii=False\n",
        "            ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Story Details:\",\n",
        "            \"```json\"\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "-G4HUq5ZJ0gd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TranslatedStory(BaseModel):\n",
        "    translated_title: str = Field(..., min_length=5, max_length=300,\n",
        "                                  description=\"Suggested translated title of the news story.\")\n",
        "    translated_content: str = Field(..., min_length=5,\n",
        "                                    description=\"Translated content of the news story.\")\n",
        "\n",
        "targeted_lang = \"English\"\n",
        "\n",
        "translation_messages = [\n",
        "    {\n",
        "        \"role\": \"system\",\n",
        "        \"content\": \"\\n\".join([\n",
        "            \"You are a professional translator.\",\n",
        "            \"You will be provided by an Arabic text.\",\n",
        "            \"You have to translate the text into the `Targeted Language`.\",\n",
        "            \"Follow the provided Scheme to generate a JSON\",\n",
        "            \"Do not generate any introduction or conclusion.\"\n",
        "        ])\n",
        "    },\n",
        "    {\n",
        "        \"role\": \"user\",\n",
        "        \"content\":  \"\\n\".join([\n",
        "            \"## Story:\",\n",
        "            story.strip(),\n",
        "            \"\",\n",
        "\n",
        "            \"## Pydantic Details:\",\n",
        "            json.dumps( TranslatedStory.model_json_schema(), ensure_ascii=False ),\n",
        "            \"\",\n",
        "\n",
        "            \"## Targeted Language:\",\n",
        "            targeted_lang,\n",
        "            \"\",\n",
        "\n",
        "            \"## Translated Story:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ])\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "xxeOS9jgJ2j1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "target_dir = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "_ = snapshot_download(\n",
        "    repo_id=\"Qwen/Qwen2.5-0.5B-Instruct\",\n",
        "    local_dir=target_dir,\n",
        "    local_dir_use_symlinks=False,\n",
        "    token=None,\n",
        ")\n",
        "\n",
        "base_model_id = target_dir\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "FC0z3i8-J6C9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import torch\n",
        "\n",
        "base_model_id = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-0.5B-Instruct\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "\n",
        "torch_dtype = torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True,\n",
        ")\n"
      ],
      "metadata": {
        "id": "sWHWQHRlP3qB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "id": "EszmL5AxKBds"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    details_extraction_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "pW1886DqMqns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response"
      ],
      "metadata": {
        "id": "Dli8QZhANCbT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "generated_ids = model.generate(\n",
        "    model_inputs.input_ids,\n",
        "    max_new_tokens=1024,\n",
        "    do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        ")\n",
        "\n",
        "generated_ids = [\n",
        "    output_ids[len(input_ids):]\n",
        "    for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "]\n",
        "\n",
        "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]"
      ],
      "metadata": {
        "id": "KDvkRiQXQgD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(response)"
      ],
      "metadata": {
        "id": "qdtn7UjTUZnk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "\n",
        "target_dir = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "_ = snapshot_download(\n",
        "    repo_id=\"Qwen/Qwen2.5-7B-Instruct\",\n",
        "    local_dir=target_dir,\n",
        "    local_dir_use_symlinks=False,\n",
        "    token=None,\n",
        "\n",
        "print(\"✅ Model downloaded to:\", target_dir)"
      ],
      "metadata": {
        "id": "u7w9awFsUc2U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "Lg20BjwXo5Zc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "\n",
        "base_model_id_t = \"/gdrive/MyDrive/finetune/mymodels/Qwen2.5-7B-Instruct\"\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id, use_fast=True)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id_t,\n",
        "    device_map=\"auto\",\n",
        "    quantization_config=bnb_config,\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "\n",
        "print(\"✅ Model loaded successfully\")"
      ],
      "metadata": {
        "id": "dmyeClzNkypv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = model.device  # cuda:0\n",
        "\n",
        "\n",
        "# translation_messages = [\n",
        "#   {\"role\": \"system\", \"content\": \"You are a helpful assistant that replies in Arabic.\"},\n",
        "#   {\"role\": \"user\", \"content\": \"ترجم للجملة التالية للإنجليزية: الذكاء الاصطناعي رائع.\"}\n",
        "# ]\n",
        "\n",
        "text = tokenizer.apply_chat_template(\n",
        "    translation_messages,\n",
        "    tokenize=False,\n",
        "    add_generation_prompt=True\n",
        ")\n",
        "\n",
        "inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.inference_mode():\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=512,\n",
        "        do_sample=False,\n",
        "        temperature=None,\n",
        "        top_p=None,\n",
        "        top_k=None,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "    )\n",
        "\n",
        "gen_ids = outputs[0, inputs[\"input_ids\"].shape[1]:]\n",
        "response = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "print(response)\n"
      ],
      "metadata": {
        "id": "whJ2ILkGoHDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cpu\"\n",
        "torch_dtype = torch.float32\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=device,\n",
        "    torch_dtype=torch_dtype,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "print(\"✅ Model loaded successfully\")\n"
      ],
      "metadata": {
        "id": "RvBwsUU3Z6i6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes\n",
        "!pip install --upgrade numpy\n"
      ],
      "metadata": {
        "id": "qDoyDEcLX8Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/finetune\"\n",
        "\n",
        "raw_data_path = join(data_dir, \"finetune\",\"datasets\", \"news-sample.jsonl\")\n",
        "\n",
        "raw_data = []\n",
        "with open(raw_data_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    for line in f:\n",
        "        if line.strip() == \"\":\n",
        "            continue\n",
        "        raw_data.append(json.loads(line.strip()))\n",
        "\n",
        "random.Random(101).shuffle(raw_data)\n",
        "\n",
        "print(f\"✅ Raw data loaded: {len(raw_data)} samples\")\n"
      ],
      "metadata": {
        "id": "eXBejS9OtIr9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_data[0]['content']"
      ],
      "metadata": {
        "id": "WKW8SSRiur5T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json, json_repair, re, os\n",
        "from os.path import join\n",
        "from tqdm.auto import tqdm\n",
        "import torch\n",
        "\n",
        "data_dir = \"/gdrive/MyDrive/finetune\"\n",
        "save_to = join(data_dir, \"finetune\",\"datasets\", \"sft.jsonl\")\n",
        "os.makedirs(os.path.dirname(save_to), exist_ok=True)\n",
        "\n",
        "prompt_tokens = 0\n",
        "completion_tokens = 0\n",
        "\n",
        "def extract_json(text: str):\n",
        "    m = re.search(r\"```json\\s*(\\{.*\\}|\\[.*\\])\\s*```\", text, flags=re.S)\n",
        "    if not m:\n",
        "        m = re.search(r\"(\\{.*\\}|\\[.*\\])\", text, flags=re.S)\n",
        "    if not m:\n",
        "        return None\n",
        "    raw = m.group(1)\n",
        "    try:\n",
        "        return json.loads(raw)\n",
        "    except Exception:\n",
        "        try:\n",
        "            return json_repair.loads(raw)\n",
        "        except Exception:\n",
        "            return None\n",
        "\n",
        "def ask_teacher(messages, max_new_tokens=512, temperature=0.2):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages, tokenize=False, add_generation_prompt=True\n",
        "    )\n",
        "    device = model.device\n",
        "    inputs = tokenizer(text, return_tensors=\"pt\").to(device)\n",
        "    input_len = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "    with torch.inference_mode():\n",
        "        out = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=(temperature is not None and temperature > 0),\n",
        "            temperature=temperature if temperature else None,\n",
        "            top_p=0.9 if temperature else None,\n",
        "            eos_token_id=tokenizer.eos_token_id,\n",
        "            pad_token_id=tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "    gen_ids = out[0, input_len:]\n",
        "    reply = tokenizer.decode(gen_ids, skip_special_tokens=True)\n",
        "\n",
        "    in_tok = inputs[\"input_ids\"].numel()\n",
        "    out_tok = gen_ids.numel()\n",
        "    return reply, in_tok, out_tok\n",
        "\n",
        "ix = 0\n",
        "for story in tqdm(raw_data):\n",
        "    sample_details_extraction_messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"You are an NLP data paraser.\",\n",
        "                \"You will be provided by an Arabic text associated with a Pydantic scheme.\",\n",
        "                \"Generate the output in the same story language.\",\n",
        "                \"You have to extract JSON details from text according the Pydantic details.\",\n",
        "                \"Extract details as mentioned in text.\",\n",
        "                \"Do not generate any introduction or conclusion.\"\n",
        "            ])\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": \"\\n\".join([\n",
        "                \"## Story:\",\n",
        "                story['content'].strip(),\n",
        "                \"\",\n",
        "                \"## Pydantic Details:\",\n",
        "                json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "                \"\",\n",
        "                \"## Story Details:\",\n",
        "                \"```json\"\n",
        "            ])\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    try:\n",
        "        llm_response, in_tok, out_tok = ask_teacher(\n",
        "            sample_details_extraction_messages,\n",
        "            max_new_tokens=512,\n",
        "            temperature=0.2\n",
        "        )\n",
        "    except RuntimeError as e:\n",
        "        if \"out of memory\" in str(e).lower():\n",
        "            torch.cuda.empty_cache()\n",
        "            llm_response, in_tok, out_tok = ask_teacher(\n",
        "                sample_details_extraction_messages,\n",
        "                max_new_tokens=256,\n",
        "                temperature=0.2\n",
        "            )\n",
        "        else:\n",
        "            continue\n",
        "\n",
        "    llm_resp_dict = extract_json(llm_response)\n",
        "    if not llm_resp_dict:\n",
        "        continue\n",
        "\n",
        "    with open(save_to, \"a\", encoding=\"utf-8\") as f:\n",
        "        f.write(json.dumps({\n",
        "            \"id\": ix,\n",
        "            \"story\": story['content'].strip(),\n",
        "            \"task\": \"Extract the story details into a JSON.\",\n",
        "            \"output_scheme\": json.dumps(NewsDetails.model_json_schema(), ensure_ascii=False),\n",
        "            \"response\": llm_resp_dict\n",
        "        }, ensure_ascii=False, default=str) + \"\\n\")\n",
        "\n",
        "    ix += 1\n",
        "    prompt_tokens += in_tok\n",
        "    completion_tokens += out_tok\n",
        "\n",
        "    if (ix % 3) == 0:\n",
        "        print(f\"Iteration {ix}: prompt_tokens={prompt_tokens}, completion_tokens={completion_tokens}\")\n",
        "\n",
        "print(f\"✅ Done. Written samples: {ix}\")\n",
        "print(f\"📄 Saved to: {save_to}\")\n"
      ],
      "metadata": {
        "id": "Yxcmay0rvrZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import random\n",
        "from os.path import join\n",
        "\n",
        "sft_data_path = join(data_dir, \"finetune\",\"datasets\", \"sft.jsonl\")\n",
        "llm_finetunning_data = []\n",
        "\n",
        "system_message = \"\\n\".join([\n",
        "    \"You are a professional NLP data parser.\",\n",
        "    \"Follow the provided `Task` by the user and the `Output Scheme` to generate the `Output JSON`.\",\n",
        "    \"Do not generate any introduction or conclusion.\"\n",
        "])\n",
        "\n",
        "for line in open(sft_data_path):\n",
        "    if line.strip() == \"\":\n",
        "        continue\n",
        "\n",
        "    rec = json.loads(line.strip())\n",
        "\n",
        "    llm_finetunning_data.append({\n",
        "        \"system\": system_message,\n",
        "        \"instruction\": \"\\n\".join([\n",
        "            \"# Story:\",\n",
        "            rec[\"story\"],\n",
        "\n",
        "            \"# Task:\",\n",
        "            rec[\"task\"],\n",
        "            \"# Output Scheme:\",\n",
        "            rec[\"output_scheme\"],\n",
        "            \"\",\n",
        "\n",
        "            \"# Output JSON:\",\n",
        "            \"```json\"\n",
        "\n",
        "        ]),\n",
        "        \"input\": \"\",\n",
        "        \"output\": \"\\n\".join([\n",
        "            \"```json\",\n",
        "            json.dumps(rec[\"response\"], ensure_ascii=False, default=str),\n",
        "            \"```\"\n",
        "        ]),\n",
        "        \"history\": []\n",
        "    })\n",
        "\n",
        "random.Random(101).shuffle(llm_finetunning_data)\n",
        "\n",
        "print(f\"Total data samples: {len(llm_finetunning_data)}\")\n"
      ],
      "metadata": {
        "id": "9nFUDwyAxNM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sample_sz = 2700\n",
        "\n",
        "train_ds = llm_finetunning_data[:train_sample_sz]\n",
        "eval_ds = llm_finetunning_data[train_sample_sz:]\n",
        "\n",
        "os.makedirs(join(data_dir, \"finetune\",\"datasets\", \"llamafactory-finetune-data\"), exist_ok=True)\n",
        "\n",
        "with open(join(data_dir, \"finetune\",\"datasets\", \"llamafactory-finetune-data\", \"train.json\"), \"w\") as dest:\n",
        "    json.dump(train_ds, dest, ensure_ascii=False, default=str)\n",
        "\n",
        "with open(join(data_dir,\"finetune\", \"datasets\", \"llamafactory-finetune-data\", \"val.json\"), \"w\", encoding=\"utf8\") as dest:\n",
        "    json.dump(eval_ds, dest, ensure_ascii=False, default=str)"
      ],
      "metadata": {
        "id": "66lKbravc5vN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n",
        "\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "# max_samples: 50\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "# resume_from_checkpoint: /gdrive/MyDrive/youtube-resources/llm-finetuning/models/checkpoint-1500\n",
        "output_dir: /gdrive/MyDrive/finetune/finetune/llm-finetuning/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "# overwrite_output_dir: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "# val_size: 0.1\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: newsx-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"abdelati88/news-analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n"
      ],
      "metadata": {
        "id": "eaVyoKgEeqk1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/hiyouga/LLaMA-Factory.git\n",
        "%cd LLaMA-Factory\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "0zYx9wU0ilmd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "DKrSDqwSi8Ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/\n"
      ],
      "metadata": {
        "id": "jDaWDNe6lPrE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/examples/train_lora/\n"
      ],
      "metadata": {
        "id": "_DMbvjm3p3jK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n",
        "!pip install -r requirements.txt\n"
      ],
      "metadata": {
        "id": "mqmJ5KDCqCLc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "waG2986JqvCx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!llama_factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "la28pzfWrX9j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/LLaMA-Factory/llama_factory_cli.py train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "WyNmBxNRro5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/LLaMA-Factory/requirements.txt\n"
      ],
      "metadata": {
        "id": "EBHKDdsJrzZo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!ls /content/LLaMA-Factory/\n"
      ],
      "metadata": {
        "id": "WP1SXThisJYo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!find . -name \"llama_factory_cli.py\"\n"
      ],
      "metadata": {
        "id": "lDV8bD7pvibM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets wandb huggingface_hub\n"
      ],
      "metadata": {
        "id": "7f5QJNsgwn_3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python3 /content/LLaMA-Factory/llama_factory_cli.py train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "oPgotCjB2oM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "id": "ZgMRPpLK2o_L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd LLaMA-Factory/ && llamafactory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml"
      ],
      "metadata": {
        "id": "dpk_1W552tji"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /content/LLaMA-Factory/examples/train_lora\n",
        "\n",
        "news_finetune_yaml = \"\"\"\n",
        "### model\n",
        "model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct\n",
        "trust_remote_code: true\n",
        "\n",
        "### method\n",
        "stage: sft\n",
        "do_train: true\n",
        "finetuning_type: lora\n",
        "lora_rank: 64\n",
        "lora_target: all\n",
        "\n",
        "### dataset\n",
        "dataset: news_finetune_train\n",
        "eval_dataset: news_finetune_val\n",
        "template: qwen\n",
        "cutoff_len: 3500\n",
        "overwrite_cache: true\n",
        "preprocessing_num_workers: 16\n",
        "\n",
        "### output\n",
        "output_dir: /content/drive/MyDrive/finetune/models/\n",
        "logging_steps: 10\n",
        "save_steps: 500\n",
        "plot_loss: true\n",
        "\n",
        "### train\n",
        "per_device_train_batch_size: 1\n",
        "gradient_accumulation_steps: 4\n",
        "learning_rate: 1.0e-4\n",
        "num_train_epochs: 3.0\n",
        "lr_scheduler_type: cosine\n",
        "warmup_ratio: 0.1\n",
        "bf16: true\n",
        "ddp_timeout: 180000000\n",
        "\n",
        "### eval\n",
        "per_device_eval_batch_size: 1\n",
        "eval_strategy: steps\n",
        "eval_steps: 100\n",
        "\n",
        "report_to: wandb\n",
        "run_name: newsx-finetune-llamafactory\n",
        "\n",
        "push_to_hub: true\n",
        "export_hub_model_id: \"abdelati88/news-analyzer\"\n",
        "hub_private_repo: true\n",
        "hub_strategy: checkpoint\n",
        "\"\"\"\n",
        "\n",
        "with open(\"/content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\", \"w\") as f:\n",
        "    f.write(news_finetune_yaml)\n"
      ],
      "metadata": {
        "id": "z7x_hani20pL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llama-factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "_bCQg4iO25VD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "HncbGbeO29Lk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "id": "nnKRDM5g3RCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n",
        "!pip install git+https://github.com/hiyouga/LLaMA-Factory.git\n"
      ],
      "metadata": {
        "id": "VHG_Gz663TZI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llama-factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "-iWzwlcg3amF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -r /content/LLaMA-Factory/requirements.txt\n",
        "\n"
      ],
      "metadata": {
        "id": "tdyVn1hD3l2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd /content/LLaMA-Factory && llama-factory-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "RvlJmdn63rKb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformers-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "WZ2Xcju03wPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/LLaMA-Factory\n",
        "huggingface-cli upload --repo_id <your-username>/<model-name> --model_path ./content/LLaMA-Factory\n"
      ],
      "metadata": {
        "id": "qGKGBgOM74dj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "id": "AScjRzdy7_Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!transformers-cli train /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "d6RRt0hO8MF8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login\n"
      ],
      "metadata": {
        "id": "yeQSDR0Z8T7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/LLaMA-Factory/src/train.py --config /content/LLaMA-Factory/examples/train_lora/news_finetune.yaml\n"
      ],
      "metadata": {
        "id": "VtE2fqpx8iK6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    base_model_id,\n",
        "    device_map=\"auto\",\n",
        "    torch_dtype = torch_dtype\n",
        ")\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(base_model_id)"
      ],
      "metadata": {
        "id": "OZOVfHSFBqLr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "finetuned_model_id = \"/gdrive/MyDrive/finetune/finetune/llm-finetuning/models\"\n",
        "model.load_adapter(finetuned_model_id)"
      ],
      "metadata": {
        "id": "_zBEiK9HK5n7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_resp(messages):\n",
        "    text = tokenizer.apply_chat_template(\n",
        "        messages,\n",
        "        tokenize=False,\n",
        "        add_generation_prompt=True\n",
        "    )\n",
        "\n",
        "    model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
        "\n",
        "    generated_ids = model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=1024,\n",
        "        do_sample=False, top_k=None, temperature=None, top_p=None,\n",
        "    )\n",
        "\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):]\n",
        "        for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "\n",
        "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "\n",
        "    return response\n",
        "\n",
        "response = generate_resp(translation_messages)"
      ],
      "metadata": {
        "id": "BtmiFA4iLmNL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}